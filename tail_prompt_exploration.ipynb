{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531560c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import diffusers\n",
    "from diffusers import AutoencoderKL, DPMSolverMultistepScheduler, StableDiffusionPipeline, UNet2DConditionModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.training_utils import EMAModel, compute_dream_and_update_latents, compute_snr\n",
    "from diffusers.utils import check_min_version, deprecate, is_wandb_available, make_image_grid\n",
    "from diffusers.utils.hub_utils import load_or_create_model_card, populate_model_card\n",
    "from diffusers.utils.import_utils import is_xformers_available\n",
    "from diffusers.utils.torch_utils import is_compiled_module\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cf2133",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"stabilityai/stable-diffusion-2-1\"\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe = pipe.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be391c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts = [\"gift\", \"horse\", \"jeep\", \"shaking hands\", \"oxygen mask\", \"pay phone\"]\n",
    "\n",
    "# for prompt in prompts:\n",
    "#     image = pipe(prompt).images[0]\n",
    "#     image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7550e98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "import timm\n",
    "\n",
    "img = Image.open(urlopen(\n",
    "    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n",
    "))\n",
    "\n",
    "model = timm.create_model('vit_so150m2_patch16_reg1_gap_256.sbb_e200_in12k_ft_in1k', pretrained=True)\n",
    "model = model.eval()\n",
    "\n",
    "# get model specific transforms (normalization, resize)\n",
    "data_config = timm.data.resolve_model_data_config(model)\n",
    "transforms = timm.data.create_transform(**data_config, is_training=False)\n",
    "\n",
    "output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n",
    "\n",
    "top5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6985f9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b2b84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"goldfish\", \"jeep\", \"oxygen mask\", \"pay phone\"]\n",
    "true_idxs = [1, 609, 691, 707]\n",
    "images_and_results = defaultdict(list)\n",
    "\n",
    "accuracies = defaultdict(list)\n",
    "num_trials = 10\n",
    "pipe.set_progress_bar_config(disable=True)\n",
    "\n",
    "\n",
    "for prompt, true_idx in zip(prompts, true_idxs):\n",
    "    for trial in range(num_trials):\n",
    "        image = pipe(prompt).images[0]\n",
    "        # image.show()\n",
    "\n",
    "        model_output = model(transforms(image).unsqueeze(0))\n",
    "        top5_probabilities, top5_class_indices = torch.topk(model_output.softmax(dim=1) * 100, k=5)\n",
    "\n",
    "        top_pred = top5_class_indices[0, 0].item()\n",
    "        accuracies[prompt].append(true_idx == top_pred)\n",
    "        images_and_results[prompt].append((image, top5_probabilities, top5_class_indices))\n",
    "\n",
    "for c in accuracies:\n",
    "    print(f\"Accuracy for class {c} = {sum(accuracies[c]) / len(accuracies[c])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252bf79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in accuracies:\n",
    "    print(f\"Accuracy for class {c} = {sum(accuracies[c]) / len(accuracies[c])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d608efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in images_and_results:\n",
    "    print(f\"Images for {prompt}\")\n",
    "\n",
    "    [i[0].show() for i in images_and_results[p]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c89a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "frequencies = [145486, 3228439, 10616, 7136]\n",
    "accs = [1.0, 1.0, 0.875, 0.525]\n",
    "labels = ['goldfish', 'jeep', 'oxygen mask', 'pay phone']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(frequencies, accs, marker='o', linewidth=2)  # numeric x- and y-axes implied\n",
    "ax.set_xlabel('n-Gram Frequency')\n",
    "ax.set_ylabel('Classifier Accuracy')\n",
    "ax.set_xscale('log')\n",
    "ax.invert_xaxis()\n",
    "for xi, yi, lab in zip(frequencies, accs, labels):\n",
    "    ax.annotate(lab, (xi, yi), xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7900eba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
